{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Packages laden"
      ],
      "metadata": {
        "id": "4_dqpQXVgm65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install regex\n",
        "!pip install -U numpy\n",
        "\n",
        "!pip install scikit-learn\n",
        "!pip install pyLDAvis\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "hLdud861gp-d",
        "outputId": "7014efdc-0b77-4133-f2ad-252ed1706823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed numpy-1.26.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.2.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Collecting numpy>=1.24.2 (from pyLDAvis)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: funcy, numpy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed funcy-2.0 numpy-1.26.4 pyLDAvis-3.4.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pfade definieren !!entsprechende Dateien hochladen!!"
      ],
      "metadata": {
        "id": "QHYGkuc0qYJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data_raw = [r'/content/4_1_ergebnisse.csv', r'/content/4_2_ergebnisse.csv', r'/content/4_3_ergebnisse.csv', r'/content/4_4_ergebnisse.csv']\n",
        "\n",
        "csv_pfad = '/content/05_csv_data_raw.csv' # nicht wundern, hatte datei zwischenzeitlich umbenannt\n",
        "\n",
        "csv_no_doubles_pfad = '/content/06_data_raw_no_doubles.csv'"
      ],
      "metadata": {
        "id": "Slx22u6-qeVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all four data subsets"
      ],
      "metadata": {
        "id": "wiKDtXQ-LvBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRZpq-UbK7Mc",
        "outputId": "97ba941a-6318-4a32-b32e-038f0681c227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "df = pd.concat((pd.read_csv(f) for f in csv_data_raw), ignore_index=True)\n",
        "\n",
        "df.to_csv('csv_data_raw.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manche Orte wurden doppelt abgerufen. Entfernung von Dopplungen in den Rezensionen."
      ],
      "metadata": {
        "id": "Ezr9gDgVgTNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entferne_doppelte_rezensionen(csv_pfad, anzahl_rezensionen=5):\n",
        "    df = pd.read_csv(csv_pfad)\n",
        "\n",
        "    # Doppelte Rezensionen basierend auf place_id und rezension entfernen\n",
        "    df = df.drop_duplicates(subset=['place_id', 'rezension'], keep='first')\n",
        "\n",
        "    # Anzahl der Rezensionen pro Ort begrenzen\n",
        "    df = df.groupby('place_id').head(anzahl_rezensionen).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Funktionsaufruf\n",
        "ergebnis_df = entferne_doppelte_rezensionen(csv_pfad)\n",
        "\n",
        "# Das bereinigte DataFrame anzeigen (optional)\n",
        "print(ergebnis_df)\n",
        "\n",
        "# Das bereinigte DataFrame in eine neue CSV-Datei speichern (optional)\n",
        "ergebnis_df.to_csv(\"06_data_raw_no_doubles.csv\", index=False)"
      ],
      "metadata": {
        "id": "BZVwj8xtghz9",
        "outputId": "e3f4c742-23aa-4bef-a64b-470438ce2a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     ort  \\\n",
            "0      DIAKO Krankenhaus gGmbH, Knuthstraße 1, Flensburg   \n",
            "1      DIAKO Krankenhaus gGmbH, Knuthstraße 1, Flensburg   \n",
            "2      DIAKO Krankenhaus gGmbH, Knuthstraße 1, Flensburg   \n",
            "3      DIAKO Krankenhaus gGmbH, Knuthstraße 1, Flensburg   \n",
            "4      DIAKO Krankenhaus gGmbH, Knuthstraße 1, Flensburg   \n",
            "...                                                  ...   \n",
            "11777  Klinikum Altenburger Land GmbH, Am Waldessaum ...   \n",
            "11778  Klinikum Altenburger Land GmbH, Am Waldessaum ...   \n",
            "11779  Klinikum Altenburger Land GmbH, Am Waldessaum ...   \n",
            "11780  Klinikum Altenburger Land GmbH, Am Waldessaum ...   \n",
            "11781  Klinikum Altenburger Land GmbH, Am Waldessaum ...   \n",
            "\n",
            "                                 name        strasze hausnr      stadt  \\\n",
            "0             DIAKO Krankenhaus gGmbH    Knuthstraße      1  Flensburg   \n",
            "1             DIAKO Krankenhaus gGmbH    Knuthstraße      1  Flensburg   \n",
            "2             DIAKO Krankenhaus gGmbH    Knuthstraße      1  Flensburg   \n",
            "3             DIAKO Krankenhaus gGmbH    Knuthstraße      1  Flensburg   \n",
            "4             DIAKO Krankenhaus gGmbH    Knuthstraße      1  Flensburg   \n",
            "...                               ...            ...    ...        ...   \n",
            "11777  Klinikum Altenburger Land GmbH  Am Waldessaum     10  Altenburg   \n",
            "11778  Klinikum Altenburger Land GmbH  Am Waldessaum     10  Altenburg   \n",
            "11779  Klinikum Altenburger Land GmbH  Am Waldessaum     10  Altenburg   \n",
            "11780  Klinikum Altenburger Land GmbH  Am Waldessaum     10  Altenburg   \n",
            "11781  Klinikum Altenburger Land GmbH  Am Waldessaum     10  Altenburg   \n",
            "\n",
            "       bundesland                     place_id  bewertung  \\\n",
            "0             1.0  ChIJU6MRELlCs0cRaU7ePDewy34          5   \n",
            "1             1.0  ChIJU6MRELlCs0cRaU7ePDewy34          5   \n",
            "2             1.0  ChIJU6MRELlCs0cRaU7ePDewy34          5   \n",
            "3             1.0  ChIJU6MRELlCs0cRaU7ePDewy34          1   \n",
            "4             1.0  ChIJU6MRELlCs0cRaU7ePDewy34          1   \n",
            "...           ...                          ...        ...   \n",
            "11777        16.0  ChIJ9yvAKoIgp0cRQCruo2WSxyU          5   \n",
            "11778        16.0  ChIJ9yvAKoIgp0cRQCruo2WSxyU          5   \n",
            "11779        16.0  ChIJ9yvAKoIgp0cRQCruo2WSxyU          5   \n",
            "11780        16.0  ChIJ9yvAKoIgp0cRQCruo2WSxyU          5   \n",
            "11781        16.0  ChIJ9yvAKoIgp0cRQCruo2WSxyU          5   \n",
            "\n",
            "                                               rezension                  zeit  \n",
            "0      Von A-Z positiv. Teil 1\\nVom Reinigungspersona...       vor einer Woche  \n",
            "1      Ich kann die vielen schlechten Rezensionen übe...          vor 3 Wochen  \n",
            "2      Ich bin total begeistert von dem Personal  auf...       vor einer Woche  \n",
            "3      Enttäuschende Erfahrung in der Notaufnahme\\n\\n...          vor 2 Wochen  \n",
            "4      Das Personal in der Aufnahme ist überfordert u...  in der letzten Woche  \n",
            "...                                                  ...                   ...  \n",
            "11777  Meine 81 Jährige Mutter wurde auf Station 24 b...          vor 2 Wochen  \n",
            "11778  Absolut zufrieden mit dem Service und vorallem...       vor einem Monat  \n",
            "11779  Ich kann jetzt nur für Kreißsaal sowie Station...         vor 2 Monaten  \n",
            "11780  Durch das schnelle und kompetente Handeln alle...         vor 2 Monaten  \n",
            "11781  Vielen lieben Dank an das gesamte Team der Sta...         vor 4 Monaten  \n",
            "\n",
            "[11782 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validierung der automatischen Abrufe: Zufallsstichprobe & manueller Abgleich"
      ],
      "metadata": {
        "id": "3kNZuQ7Ll3F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_no_doubles_pfad) #siehe oben\n",
        "\n",
        "def zufallsstichprobe(csv_pfad, stichprobengroesse=100, zufalls_seed=42): #seed 42\n",
        "    df = pd.read_csv(csv_no_doubles_pfad)\n",
        "\n",
        "    # Zufallsstichprobe ziehen\n",
        "    stichprobe_df = df.sample(n=min(stichprobengroesse, len(df)), random_state=zufalls_seed)\n",
        "\n",
        "    return stichprobe_df\n",
        "\n",
        "# Beispielaufruf:\n",
        "csv_datei = csv_no_doubles_pfad # Ersetzen Sie dies durch den tatsächlichen Dateinamen\n",
        "stichprobe = zufallsstichprobe(csv_datei, stichprobengroesse=100, zufalls_seed=42)\n",
        "\n",
        "# Die Stichprobe in eine neue CSV-Datei speichern (optional):\n",
        "stichprobe.to_csv(\"07_stichprobe_validierung.csv\", index=False)\n",
        "\n",
        "# Validierung manuell durchgeführt, für Kriterien siehe Hausarbeit;\n",
        "# Kodierung: 1 = valid; 0 = not valid or valid with restrictions;\n",
        "\n",
        "\n",
        "########## hier noch die Ergebnisse ausrechnen ###############"
      ],
      "metadata": {
        "id": "FijvKwsZl-Km",
        "outputId": "77c2fb07-3915-4c6e-b806-7d0bfbb8d10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     ort  \\\n",
            "2507   Alexius/Josef Krankenhaus, Montanusstraße 54, ...   \n",
            "11482  Sophien- und Hufeland-Klinikum gGmbH, Henry-va...   \n",
            "4207   Ev. Krankenhaus Lippstadt, Wiedenbrücker Straß...   \n",
            "1935   Rotes-Kreuz-Krankenhaus Bremen gGmbH, St.-Paul...   \n",
            "7768   Klinik Dr. Wilhelm, Hans-Urmiller-Ring 46, Wol...   \n",
            "...                                                  ...   \n",
            "10769  Hauptstandort Rudolf-Virchow-Klinikum, Virchow...   \n",
            "10563  Sana HANSE-Klinikum Wismar GmbH, Dr.-Unruh-Str...   \n",
            "8830   Bezirksklinikum Ansbach, Paracelsusstraße 36, ...   \n",
            "5006   Helios Kliniken Kassel, Hansteinstraße 29, Kassel   \n",
            "4216       LWL-Klinik Lippstadt, Im Hofholz 6, Lippstadt   \n",
            "\n",
            "                                        name                    strasze  \\\n",
            "2507               Alexius/Josef Krankenhaus             Montanusstraße   \n",
            "11482   Sophien- und Hufeland-Klinikum gGmbH  Henry-van-de-Velde-Straße   \n",
            "4207               Ev. Krankenhaus Lippstadt       Wiedenbrücker Straße   \n",
            "1935    Rotes-Kreuz-Krankenhaus Bremen gGmbH            St.-Pauli-Deich   \n",
            "7768                      Klinik Dr. Wilhelm         Hans-Urmiller-Ring   \n",
            "...                                      ...                        ...   \n",
            "10769  Hauptstandort Rudolf-Virchow-Klinikum                Virchowstr.   \n",
            "10563        Sana HANSE-Klinikum Wismar GmbH           Dr.-Unruh-Straße   \n",
            "8830                 Bezirksklinikum Ansbach           Paracelsusstraße   \n",
            "5006                  Helios Kliniken Kassel             Hansteinstraße   \n",
            "4216                    LWL-Klinik Lippstadt                 Im Hofholz   \n",
            "\n",
            "      hausnr                 stadt  bundesland                     place_id  \\\n",
            "2507      54          Grevenbroich         5.0  ChIJw7FzZxBMv0cRjlEgBW8E8q8   \n",
            "11482      2                Weimar        16.0  ChIJsXwo-S0FpEcRFkfrck38uL4   \n",
            "4207      33             Lippstadt         5.0  ChIJN-EgMv_Su0cR59EAVFIp7Og   \n",
            "1935     NaN                Bremen         4.0  ChIJh7ETvx4osUcRyTTKEulnwVw   \n",
            "7768      46        Wolfratshausen         9.0  ChIJzyXJjKPGnUcR6pbD81dI7JU   \n",
            "...      ...                   ...         ...                          ...   \n",
            "10769     18              Glauchau        14.0  ChIJ0a9ezKQvp0cR9LyxJpurJao   \n",
            "10563     14                Wismar        13.0  ChIJJzo-TVjHrUcRswybCjK0bIU   \n",
            "8830      36  Neustadt a. d. Aisch         9.0  ChIJn58c59kLokcRVMusIFGWHLw   \n",
            "5006      29                Kassel         6.0  ChIJtwosb2U_u0cRF_gesuG38XE   \n",
            "4216       6             Lippstadt         5.0  ChIJ_fI8VCTUu0cRnlWefvOgJPQ   \n",
            "\n",
            "       bewertung                                          rezension  \\\n",
            "2507           5  Bin die fünfte Woche in der Tagesklinik St. Au...   \n",
            "11482          5  Ich mußte eine OP in der Tagesklinik machen la...   \n",
            "4207           1  Leider wurde uns in der Notfallambulanz überha...   \n",
            "1935           5  Mein Vater wurde letzte Woche an der Hals-Ober...   \n",
            "7768           5  Ich war wegen einer OP durch meine Chirurgin i...   \n",
            "...          ...                                                ...   \n",
            "10769          5  Haben hier vor kurzem entbunden. Alles war top...   \n",
            "10563          1  Wer die Möglichkeit besitzt sollte dringend ei...   \n",
            "8830           5  Beste Einrichtung im Universum\\nAls Ich hier a...   \n",
            "5006           1  Mein Mann wurde morgens etwa um 10 Uhr aufgrun...   \n",
            "4216           5  Es war im Jahr 2020. Nichts ging mehr. Nicht g...   \n",
            "\n",
            "                       zeit  \n",
            "2507           vor 2 Jahren  \n",
            "11482          vor 3 Wochen  \n",
            "4207   in der letzten Woche  \n",
            "1935          vor 3 Monaten  \n",
            "7768        vor einem Monat  \n",
            "...                     ...  \n",
            "10769         vor 2 Monaten  \n",
            "10563       vor einer Woche  \n",
            "8830         vor 10 Monaten  \n",
            "5006        vor einem Monat  \n",
            "4216           vor 2 Jahren  \n",
            "\n",
            "[100 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning - Code aus dem CSS Buch"
      ],
      "metadata": {
        "id": "vYUB3NeIsadS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aus CSS Buch, Kapitel 9.1:\n",
        "\n",
        "text = \"\"\"   <b>Communication</b>\n",
        "    (from Latin communicare, meaning to share) \"\"\"\n",
        "# remove tags:\n",
        "cleaned = text.replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n",
        "# normalize white space\n",
        "cleaned = \" \".join(cleaned.split())\n",
        "# lower case\n",
        "cleaned = cleaned.lower()\n",
        "# trim spaces from start and end\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "print(cleaned)\n",
        "\n",
        "# aus CSS Buch, Kapitel 9.3 RegEx:\n",
        "text = \"\"\"   <b>Communication</b>\n",
        "    (from Latin communicare, meaning to share) \"\"\"\n",
        "# remove tags:\n",
        "cleaned = re.sub(\"<[^>]+>\", \"\", text)\n",
        "# normalize white space\n",
        "cleaned = re.sub(\"\\s+\", \" \", cleaned)\n",
        "# trim spaces from start and end\n",
        "cleaned = re.sub(\"^\\s+|\\s+$\", \"\", cleaned)\n",
        "cleaned = cleaned.strip()\n",
        "\n",
        "print(cleaned)\n",
        "\n",
        "# regex on a dataframe\n",
        "url = \"https://cssbook.net/d/example_tweets.csv\"\n",
        "tweets = pd.read_csv(url, index_col=\"id\")\n",
        "# identify tweets with hashtags\n",
        "tweets[\"tag\"] = tweets.text.str.contains(r\"#\\w+\")\n",
        "# How many at-mentions are there?\n",
        "tweets[\"at\"] = tweets.text.str.count(r\"(^|\\s)@\\w+\")\n",
        "# Extract first url\n",
        "tweets[\"url\"] = tweets.text.str.extract(r\"(https?://\\S+)\")\n",
        "# Remove urls, tags, and @-mentions\n",
        "expr = r\"(^|\\s)(@|#|https?://)\\S+\"\n",
        "tweets[\"plain2\"] = tweets.text.str.replace(expr, \" \", regex=True).replace(\n",
        "    r\"\\W+\", \" \"\n",
        ")\n",
        "tweets\n",
        "\n",
        "# 9.3.1 Splitting extracting and joining a single text\n",
        "text = \"apples, pears, oranges\"\n",
        "# Three ways to achieve the same thing:\n",
        "items = text.split(\", \")\n",
        "items = regex.split(r\"\\p{PUNCTUATION}\\s*\", text)\n",
        "items = regex.findall(r\"\\p{LETTER}+\", text)\n",
        "print(f\"Split text into items: {items}\")\n",
        "\n",
        "joined = \" & \".join(items)\n",
        "print(joined)\n",
        "\n",
        "# Applying split and extract _ all on text columns\n",
        "tags = tweets.text.str.extractall(\"(#\\\\w+)\")\n",
        "tags.merge(tweets, left_on=\"id\", right_on=\"id\")\n",
        "\n",
        "\n",
        "\n",
        "###### hier auf jeden fall noch richtigen code reinballern"
      ],
      "metadata": {
        "id": "DThy4dIKmll5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing mit Packets"
      ],
      "metadata": {
        "id": "BiFC4bxYJwLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def split_by_paragraph(text): # Lange Rezensionen nach Paragraphen zerstückeln\n",
        "    return text.split('\\n\\n')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Nur Buchstaben, Zahlen oder Leerzeichen;\n",
        "    text = text.lower() # Lowercasing;\n",
        "    tokens = word_tokenize(text) # Tokenization\n",
        "    stop_words = set(stopwords.words('german')) # deutsche stopwords benutzten, da Rezensionen auf deutsch\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2] # Entfernung von stopwords und extrem kurzen Wörtern\n",
        "    return tokens\n",
        "\n",
        "# Ausführung\n",
        "df = pd.read_csv(r'/content/06_data_raw_no_doubles.csv')\n",
        "\n",
        "df['paragraphs'] = df['rezension'].astype(str).apply(split_by_paragraph)\n",
        "\n",
        "df['tokens'] = df['rezension'].astype(str).apply(preprocess_text)"
      ],
      "metadata": {
        "id": "hWfnjO4tJzaa",
        "outputId": "11b2b5c1-4f30-4562-cdf6-7749c9d87dd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text-as-Data-Preparation"
      ],
      "metadata": {
        "id": "xETxXmS5xdb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"computations are usually done on numerical data – but you have text. Hence, you must find a way to represent the text by numbers.\"\n",
        "# dtm = numerical representation of text\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join tokens back into sentences\n",
        "df['processed_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['processed_text'])"
      ],
      "metadata": {
        "id": "dH4LT0pAxgHs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA Analysis und erste Visualisierung"
      ],
      "metadata": {
        "id": "a-6XLHZ_Kin-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "!pip install pyLDAvis==3.3.1\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn_api\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Adjust n_components\n",
        "lda.fit(X)\n",
        "\n",
        "# Visualisierung\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn_api.prepare(lda, X, vectorizer, mds='tsne')\n",
        "pyLDAvis.save_html(panel, 'lda.html') #Speichert die Visualisierung als html\n",
        "\n",
        "# Top Topics Anzeigen\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 10\n",
        "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)"
      ],
      "metadata": {
        "id": "CSo8Xd4_KmLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "7a4e8e27-5dae-43b3-fe0a-ede180426377"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==3.3.1\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.7 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (2.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (2.10.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (1.0.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.3.1) (2.0)\n",
            "Collecting sklearn (from pyLDAvis==3.3.1)\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyLDAvis.sklearn_api'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d2e7aae6668d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyLDAvis==3.3.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn_api'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n_components bzw k festlegen"
      ],
      "metadata": {
        "id": "SNWhvh0aLyds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Daten laden und preprocessen (siehe vorherige Beispiele)\n",
        "\n",
        "# Define Parameter Grid\n",
        "param_grid = {'n_components': [2, 5, 10, 15]}\n",
        "\n",
        "# LDA Model\n",
        "lda = LatentDirichletAllocation(random_state=42)\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(lda, param_grid, scoring='neg_log_perplexity', cv=3)\n",
        "grid_search.fit(X)\n",
        "\n",
        "# Bestes Modell\n",
        "best_lda = grid_search.best_estimator_\n",
        "print(\"Beste Anzahl an Topics:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "WINmnHnxL011"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KOMPLETTER TEST HIER:"
      ],
      "metadata": {
        "id": "4MdBrb3Q2wUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pyLDAvis.gensim_models  # Wichtig: gensim_models importieren\n",
        "import pyLDAvis\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    stop_words = set(stopwords.words('german'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "    return tokens\n",
        "\n",
        "def prepare_corpus(processed_docs):\n",
        "    dictionary = corpora.Dictionary(processed_docs)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "    return dictionary, corpus\n",
        "\n",
        "def train_lda(corpus, dictionary, num_topics=5):\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
        "    return lda_model\n",
        "\n",
        "def evaluate_model(lda_model, corpus, dictionary, processed_docs):\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence = coherence_model_lda.get_coherence()\n",
        "    print('\\nCoherence Score: ', coherence)\n",
        "    return coherence\n",
        "\n",
        "def display_topics(lda_model, num_words=10):\n",
        "    for idx, topic in lda_model.print_topics(-1, num_words=num_words):\n",
        "        print(f'Topic: {idx} \\nWords: {topic}')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(r'/content/06_data_raw_no_doubles.csv')\n",
        "review_column = 'rezension'\n",
        "if review_column not in df.columns:\n",
        "    raise ValueError(f\"Column '{review_column}' not found in DataFrame.\")\n",
        "\n",
        "# Preprocess data\n",
        "processed_docs = df[review_column].astype(str).map(preprocess_text)\n",
        "\n",
        "# Prepare corpus\n",
        "dictionary, corpus = prepare_corpus(processed_docs)\n",
        "\n",
        "# Train LDA model\n",
        "num_topics = 5  # Adjust as needed\n",
        "lda_model = train_lda(corpus, dictionary, num_topics)\n",
        "\n",
        "# Evaluate model\n",
        "coherence_score = evaluate_model(lda_model, corpus, dictionary, processed_docs)\n",
        "\n",
        "# Display topics\n",
        "display_topics(lda_model)\n",
        "\n",
        "# Visualisierung mit pyLDAvis\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary) # prepare aus gensim_models\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "ir2eUyMp2yJR",
        "outputId": "fbc2f424-65df-4928-e7c6-ddb8ef4ca5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a58f23b851ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoherencemodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    }
  ]
}